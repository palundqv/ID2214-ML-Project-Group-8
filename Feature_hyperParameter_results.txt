Random Forest x SelectKBest unbalanced
5 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}, 'ROC-AUC': 0.5586991086485185}]]
10 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}, 'ROC-AUC': 0.5717754757889665}]]
30 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}, 'ROC-AUC': 0.6357793302818597}]]
50 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': False, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}, 'ROC-AUC': 0.6486437003131775}]]
80 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}, 'ROC-AUC': 0.6414936159961455}]]
100 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}, 'ROC-AUC': 0.6287256082871597}]]
150 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}, 'ROC-AUC': 0.5998458202842689}]]
200 : [[{'Model': 'Random Forest', 'Best Parameters:': {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}, 'ROC-AUC': 0.6265670922669236}]]


Random forest 50 features balanced dataset 1784 x 1784
RandomForestClassifier(bootstrap=True, max_depth=40, min_samples_leaf=2,min_samples_split=2,n_estimators=200,class_weight="balanced")
Confusion Matrix:
[[1236  548]
 [ 505 1279]]
AUC-ROC Score: 0.7739985094210622


***************************************************************************************************************************************************************************************************

K-nearest gave shit results

***************************************************************************************************************************************************************************************************

MLP-Classifier x SelectKBest Balanced set
10 : [[{'Model': 'Multi-layer Perceptron', 'Best Parameters:': {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50,)}, 'ROC-AUC': 0.647604963762303}]]
30 : [[{'Model': 'Multi-layer Perceptron', 'Best Parameters:': {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,)}, 'ROC-AUC': 0.7030113212285243}]]
50 : [[{'Model': 'Multi-layer Perceptron', 'Best Parameters:': {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}, 'ROC-AUC': 0.6787610125628667}]]
80 : [[{'Model': 'Multi-layer Perceptron', 'Best Parameters:': {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50,)}, 'ROC-AUC': 0.7063316568893476}]]
100 : [[{'Model': 'Multi-layer Perceptron', 'Best Parameters:': {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50, 50)}, 'ROC-AUC': 0.6898311232639738}]]

***************************************************************************************************************************************************************************************************

UnderSampled results with and without smote for random forest and MPL. 

New Label Counts:
ACTIVE
0.0    1784
1.0    1784
Name: count, dtype: int64
Random-Forest with SMOTE:  0.7580369980293189
Random-Forest no SMOTE:  0.7580369980293189
MLP with SMOTE:  0.6778090374831587
MLP no SMOTE:  0.6809985119346862

***************************************************************************************************************************************************************************************************
Oversampeled with SMOTE Suspect issue with smote samples creeping into testdata

Random-Forest with SMOTE:  0.6913943502619451
[[4.99196667e+04 5.62333333e+02]
 [5.59666667e+02 3.50000000e+01]]
Random-Forest no SMOTE:  0.7564720692679133
[[34530.66666667 15951.33333333]
 [  176.33333333   418.33333333]]

MLP with SMOTE:  0.6303759586855833
[[48918.33333333  1563.66666667]
 [  538.66666667    56.        ]]
MLP no SMOTE:  0.704948103561898
[[32315.66666667 18166.33333333]
 [  246.66666667   348.        ]]
